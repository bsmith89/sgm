import pandas as pd
from lib.snake import alias_recipe, alias_fmt, curl_recipe, curl_unzip_recipe
import glob
import os.path

wildcard_constraints:
    genome_id='[^.]*',
    sample_id='[^.]*',
    nreads='[0-9e]*',
    seed='[0-9]{1,4}',
    community='[^.]*'

_genome = pd.read_table(config['_genome'], index_col='genome_id')
config['ftp_stem'] = _genome.ftp_stem.to_dict()

config['community'] = {}
_community_dir = config['_community_dir']
for path in glob.glob(os.path.join(_community_dir, '*.tsv')):
    basename = os.path.basename(path)[:-4]
    _community = pd.read_table(path)
    config['community'][basename] = _community.genome_id.tolist()

rule download_ncbi_taxdump:
    output: 'raw/ref/new_taxdump.tar.gz'
    params:
        url='https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/new_taxdump.zip'
    shell: curl_recipe

rule unpack_ncbi_taxonomy:
    output: 'ref/ncbi_taxonomy.tsv'
    input: 'raw/ref/new_taxdump.tar.gz'
    shell:
        r'''
        tmpdir=$(mktemp -d)
        echo $tmpdir
        tar -C $tmpdir -xzvf {input}
        cat $tmpdir/rankedlineage.dmp | sed 's:\t|\t\?:\t:g' | awk -v FS='\t' -v OFS='\t' '{{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10}}' > {output}
        '''

rule download_refseq_genome:
    output: 'raw/genome/{genome}.fn'
    params:
        url=lambda w: config['ftp_stem'][w.genome] + '_genomic.fna.gz'
    resources:
        net_requests_per_second=1
    shell:
        '''
        sleep 1
        echo "Downloading {wildcards.genome} from {params.url}"
        curl {params.url} | zcat > {output}
        grep --quiet '^>' {output} || (echo "No FASTA records found in {output}" && false)
        '''

rule link_reference_genome:
    output: 'ref/genome/{genome}.fn'
    input: 'raw/genome/{genome}.fn'
    shell: alias_recipe

rule simulate_community_mgen:
    output:
        r1='data/sim/{community}.n{nreads}.s{seed}.m.r1.fq.gz',
        r2='data/sim/{community}.n{nreads}.s{seed}.m.r2.fq.gz',
        abund='data/sim/{community}.n{nreads}.s{seed}.m.abund.tsv'
    input:
        script='scripts/simulate_community_mgen.py',
        replicon='data/replicon.tsv',
        community='meta/sim/community/{community}.tsv',
        genomes=lambda w: expand('ref/genome/{genome}.fn', genome=config['community'][w.community])
    params:
        genomes_dir='ref/genome/'
    shell:
        '''
        pipe_r1=$(mktemp -u) && mkfifo $pipe_r1
        pipe_r2=$(mktemp -u) && mkfifo $pipe_r2
        gzip -c < $pipe_r1 > {output.r1} & pid1=$!
        gzip -c < $pipe_r2 > {output.r2} & pid2=$!
        {input.script} {input.community} {wildcards.community} {input.replicon} \
                {params.genomes_dir} {wildcards.nreads} {wildcards.seed} \
                {output.abund} $pipe_r1 $pipe_r2
        for pid in $pid1 $pid2
        do
            wait $pid || [ "$?" -eq 127 ]
        done
        '''

rule quality_trim_reads:
    output:
        r1='data/sim/{stem}.qtrim.r1.fq.gz',
        r2='data/sim/{stem}.qtrim.r2.fq.gz',
        r3='data/sim/{stem}.qtrim.r3.fq.gz'
    input:
        r1='data/sim/{stem}.r1.fq.gz',
        r2='data/sim/{stem}.r2.fq.gz'
    params:
        qual_type='sanger',
        qual_thresh=20
    shell:
        '''
        sickle pe -t {params.qual_type} -q {params.qual_thresh} --gzip-output \
            --pe-file1 {input.r1} --pe-file2 {input.r2} \
            --output-pe1 {output.r1} --output-pe2 {output.r2} \
            --output-single {output.r3}
        '''

# {seed} here is not the simulated mgen seed, but the seed for the simulated SET of reads.
rule assemble_mgen_samples:
    output:
        fasta='data/sim/{community}.n{nreads}.z{nsamples}.s{seed}.a.fn',
        fastg='data/sim/{community}.n{nreads}.z{nsamples}.s{seed}.a.fg',
    input:
        r1=lambda w: [f'data/sim/{{community}}.n{{nreads}}.s{seed:04}.m.qtrim.r1.fq.gz'
                      for seed
                      in range(int(w.nsamples) * int(w.seed),
                               int(w.nsamples) * (int(w.seed) + 1)
                              )],
        r2=lambda w: [f'data/sim/{{community}}.n{{nreads}}.s{seed:04}.m.qtrim.r2.fq.gz'
                      for seed
                      in range(int(w.nsamples) * int(w.seed),
                               int(w.nsamples) * (int(w.seed) + 1)
                              )],
    params:
        r1_list=lambda w, input: ','.join(input.r1),
        r2_list=lambda w, input: ','.join(input.r2),
        k_list='21,41,61,81,101',
        k_max=101,
        outdir='data/sim/{community}.n{nreads}.z{nsamples}.s{seed}.a.d'
    threads: MAX_THREADS
    log: 'log/{community}.n{nreads}.z{nsamples}.s{seed}.a.log'
    shell:
        '''
        tmpdir=$(mktemp -du)
        echo "Running MEGAHIT with outdir $tmpdir"
        megahit \
            -1 {params.r1_list} \
            -2 {params.r2_list} \
            --k-list {params.k_list} \
            --out-dir $tmpdir \
            --num-cpu-threads {threads} \
            --verbose \
            2>&1 | tee {log}
        cp $tmpdir/final.contigs.fa {output.fasta}
        megahit_toolkit contig2fastg {params.k_max} $tmpdir/intermediate_contigs/k{params.k_max}.contigs.fa > {output.fastg}
        '''

rule make_community_blastdb:
    output:
        nhr='ref/sim/{community}.fn.nhr',
        nin='ref/sim/{community}.fn.nin',
        nsq='ref/sim/{community}.fn.nsq'
    input:
        genomes=lambda w: expand('ref/genome/{genome}.fn', genome=config['community'][w.community])
    params:
        title='ref/sim/{community}.fn'
    shell:
        '''
        cat {input} \
            | makeblastdb -dbtype nucl -in - -title {params.title} -out {params.title}
        '''

rule blastn_contigs_to_refs:
    output: 'data/sim/{community}.{stem}.blastn-refs.tsv',
    input:
        contigs='data/sim/{community}.{stem}.fn',
        nhr='ref/sim/{community}.fn.nhr',
        nin='ref/sim/{community}.fn.nin',
        nsq='ref/sim/{community}.fn.nsq'
    params:
        ref_name='ref/sim/{community}.fn',
        perc_ident=95
    threads: 3
    shell:
        '''
        blastn -num_threads {threads} -db {params.ref_name} -query {input.contigs} -perc_identity {params.perc_ident} -outfmt 6 > {output}
        '''

rule assign_contig_genome:
    output: 'data/sim/{stem}.inferred-genome.tsv'
    input:
        script='scripts/assign_genome.sh',
        blastn='data/sim/{stem}.blastn-refs.tsv',
        replicon='data/replicon.tsv'
    params:
        min_ident=98,
        min_len=1000
    shell:
        r'''
        {input.script} {params.min_ident} {params.min_len} {input.blastn} {input.replicon} > {output}
        '''


rule bowtie_index_build:
    output:
        'data/{stem}.1.bt2',
        'data/{stem}.2.bt2',
        'data/{stem}.3.bt2',
        'data/{stem}.4.bt2',
        'data/{stem}.rev.1.bt2',
        'data/{stem}.rev.2.bt2'
    input: 'data/{stem}.fn'
    log: 'log/{stem}.bowtie2-build.log'
    shell:
        """
        bowtie2-build {input} data/{wildcards.stem} >{log} 2>&1
        """

rule map_reads_to_metagenome_assembly:
    output: 'data/sim/{community}.n{nreads}.s{seed1}.m.z{nsamples}.s{seed2}.a.backmap.sort.bam'
    input:
        r1='data/sim/{community}.n{nreads}.s{seed1}.m.qtrim.r1.fq.gz',
        r2='data/sim/{community}.n{nreads}.s{seed1}.m.qtrim.r2.fq.gz',
        inx_1='data/sim/{community}.n{nreads}.z{nsamples}.s{seed2}.a.1.bt2',
        inx_2='data/sim/{community}.n{nreads}.z{nsamples}.s{seed2}.a.1.bt2',
        inx_3='data/sim/{community}.n{nreads}.z{nsamples}.s{seed2}.a.1.bt2',
        inx_4='data/sim/{community}.n{nreads}.z{nsamples}.s{seed2}.a.1.bt2',
        inx_rev1='data/sim/{community}.n{nreads}.z{nsamples}.s{seed2}.a.rev.1.bt2',
        inx_rev2='data/sim/{community}.n{nreads}.z{nsamples}.s{seed2}.a.rev.2.bt2'
    threads: 2
    params:
        index_stem=lambda w: f'data/sim/{w.community}.n{w.nreads}.z{w.nsamples}.s{w.seed2}.a',
        library_name=lambda w: f'{w.community}.n{w.nreads}.s{w.seed1}'
    shell:
        r"""
        tmp=$(mktemp)
        echo "Writing temporary bamfile to $tmp for {output}"
        bowtie2 --threads {threads} \
                -x {params.index_stem} \
                --rg-id {params.library_name} \
                -1 {input.r1} -2 {input.r2} \
            | samtools view -@ {threads} -b - \
            > $tmp
        samtools sort -@ {threads} --output-fmt=BAM -o {output} $tmp
        rm $tmp
        """

rule count_seq_lengths_nucl:
    output: 'data/sim/{stem}.nlength.tsv'
    input: script='scripts/count_seq_lengths.py', seqs='data/sim/{stem}.fn'
    shell:
        r"""
        {input.script} {input.seqs} > {output}
        """

rule calculate_mapping_depth:
    output: 'data/sim/{stem}.depth.tsv'
    input: 'data/sim/{stem}.sort.bam'
    shell:
        """
        samtools depth -d 0 {input} > {output}
        """

rule estimate_contig_cvrg:
    output: 'data/sim/{community}.n{nreads}.s{seed1}.m.z{nsamples}.s{seed2}.a.backmap.cvrg.tsv'
    input:
        script='scripts/estimate_contig_coverage.py',
        depth='data/sim/{community}.n{nreads}.s{seed1}.m.z{nsamples}.s{seed2}.a.backmap.depth.tsv',
        length='data/sim/{community}.n{nreads}.z{nsamples}.s{seed2}.a.nlength.tsv'
    params:
        float_fmt='%.6g'
    shell:
        """
        {input.script} {input.depth} {input.length} {params.float_fmt} > {output}
        """

rule combine_abund:
    output: 'data/sim/{community}.n{nreads}.z{nsamples}.s{seed}.a.abund.tsv'
    input:
        lambda w: [f'data/sim/{{community}}.n{{nreads}}.s{seed:04}.m.abund.tsv'
                   for seed
                   in range(int(w.nsamples) * int(w.seed),
                            int(w.nsamples) * (int(w.seed) + 1)
                           )]
    shell:
        '''
        for file in {input}
        do
            awk -v OFS='\t' -v sample_id=$(basename --suffix .m.abund.tsv $file) '{{print sample_id, $1, $2}}' $file
        done > {output}
        '''

rule combine_cvrg:
    output: 'data/sim/{community}.n{nreads}.z{nsamples}.s{seed}.a.backmap.cvrg.tsv'
    input:
        script='scripts/concat_tables.py',
        tables=lambda w: [f'data/sim/{{community}}.n{{nreads}}.s{seed1:04}.m.z{{nsamples}}.s{{seed}}.a.backmap.cvrg.tsv'
                          for seed1
                          in range(int(w.nsamples) * int(w.seed),
                                  int(w.nsamples) * (int(w.seed) + 1)
                                  )],
    params:
        suffix=lambda w: f'.m.z{w.nsamples}.s{w.seed}.a.backmap.cvrg.tsv'
    shell:
        r"""
        for file in {input.tables}; do
            awk -v OFS='\t' -v library_id=$(basename --suffix {params.suffix} $file) 'FNR > 1 {{print library_id, $0}}' $file
        done > {output}
        """

rule count_tetramers:
    output: '{stem}.k{klen}.tsv'
    input: script='scripts/count_kmers.py', fasta='{stem}.fn'
    shell:
        '''
        {input.script} {wildcards.klen} < {input.fasta} > {output}
        '''

rule reduce_contig_binning_dims:
    output: '{stem}.a.pca.tsv'
    input:
        script='scripts/reduce_binning_dims.py',
        nlength='{stem}.a.nlength.tsv',
        kmers='{stem}.a.k4.tsv',
        cvrg='{stem}.a.backmap.cvrg.tsv'
    params:
        min_length=1000
    shell:
        '''
        {input.script} {input.nlength} {input.kmers} {input.cvrg} {params.min_length} > {output}
        '''

rule cluster_contigs:
    output:
        out='data/{stem}.a.cluster.tsv',
        summary='data/{stem}.a.cluster.summary.tsv',
    input:
        script='scripts/cluster_contigs.py',
        pca='data/{stem}.a.pca.tsv',
        length='data/{stem}.a.nlength.tsv'
    params:
        min_contig_length=200,
        frac=0.75,
        alpha=1,  # TODO: Consider this parameter (use to be 1)
        max_clusters=100,
        seed=1
    threads: 20
    shell:
        r"""
        export MKL_NUM_THREADS={threads}
        export OPENBLAS_NUM_THREADS={threads}
        export NUMEXPR_NUM_THREADS={threads}
        export OMP_NUM_THREADS={threads}
        export VECLIB_MAXIMUM_THREADS={threads}
        {input.script} {input.pca} {input.length} \
                --min-length {params.min_contig_length} \
                --frac {params.frac} \
                --max-nbins {params.max_clusters} \
                --alpha {params.alpha} \
                --seed {params.seed} \
                --summary {output.summary} \
                > {output.out}
        """


rule quality_asses_assembly:
    output:
        directory('data/sim/{community}.n{nreads}.z{nsamples}.s{seed}.a.quast.d')
    input:
        contigs='data/sim/{community}.n{nreads}.z{nsamples}.s{seed}.a.fn',
        genomes=lambda w: expand('ref/genome/{genome}.fn', genome=config['community'][w.community])
    params:
        genomes_list=lambda w, input: ','.join(input.genomes)
    threads: 8
    shell:
        '''
        tmpdir=$(mktemp -d)
        echo "Building {output} at $tmpdir ."
        metaquast.py --threads {threads} -r {params.genomes_list} --output-dir $tmpdir {input.contigs}
        mv $tmpdir {output}
        '''
